{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Type of MS\n",
    "\n",
    "First, we import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the free text and MS Type Label\n",
    "\n",
    "Code adapted from Matt's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset into a pandas data-frame\n",
    "df_all = pd.read_csv(\"FreeTextProcessed_20220126.csv\")\n",
    "\n",
    "# Extract entries which contain responses to the free text question\n",
    "# of the questionaire \n",
    "df_free_txt = df_all[df_all[\"QuestionnaireKey\"] == \"v3_unique_textbox\"]\n",
    "\n",
    "# Extract the column which only contains the free text responses\n",
    "# Convert to np.array (because I like to)\n",
    "df_free_txt_only = df_free_txt.filter([\"Value\"]).fillna(' ').to_numpy()\n",
    "df_ms_at_diagnosis = df_free_txt.filter([\"MSAtDiagnosis\"]).fillna('Unknown').to_numpy()\n",
    "df_ms_now = df_free_txt.filter([\"MSTypeNow\"]).fillna('Unknown').to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate their shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free text shape: (6768, 1)\n",
      "MS at diagnosis shape: (6768, 1)\n",
      "MS now shape: (6768, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Free text shape:\", df_free_txt_only.shape)\n",
    "print(\"MS at diagnosis shape:\", df_ms_at_diagnosis.shape)\n",
    "print(\"MS now shape:\", df_ms_now.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling the MS Type\n",
    "\n",
    "Here, we can  use the MS Type now column, unless it is Unknown, in which case we use the one at diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_label_correspondances = {\n",
    "    0: 'PPMS',\n",
    "    1: 'SPMS',\n",
    "    2: 'RRMS',\n",
    "    3: 'Benign',\n",
    "    4: 'Unknown'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms_label = []\n",
    "\n",
    "for i in range(df_ms_now.size):\n",
    "    if df_ms_now[i].item() == 'Unknown':\n",
    "        ms_label.append(df_ms_at_diagnosis[i].item())\n",
    "    else:\n",
    "        ms_label.append(df_ms_now[i].item())\n",
    "\n",
    "ms_label_int = []\n",
    "inverted_labels = {v:k for k,v in Class_label_correspondances.items()}\n",
    "\n",
    "for MSType in ms_label:\n",
    "    ms_label_int.append(inverted_labels[MSType])\n",
    "\n",
    "free_text = []\n",
    "for i in range(df_ms_now.size):\n",
    "    free_text.append(df_free_txt_only[i].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the unknown ones into a different set\n",
    "dataset, unknown = [], []\n",
    "data_label, unknown_label = [], []\n",
    "for i in range(len(ms_label_int)):\n",
    "    (unknown, dataset)[ms_label_int[i]<4].append(free_text[i])\n",
    "    (unknown_label, data_label)[ms_label_int[i]<4].append(ms_label_int[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into train and test set \n",
    "Here I chose to use an 80/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(lendata, percentage = 0.8):\n",
    "    indexes = np.array(range(0,lendata))\n",
    "    np.random.shuffle(indexes)\n",
    "    train_size = int(lendata*percentage)\n",
    "    \n",
    "    idxs_train = indexes[:train_size]\n",
    "    idxs_test = indexes[train_size:]\n",
    "    \n",
    "    return idxs_train, idxs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENDATA = len(dataset)\n",
    "np.random.seed(69420)\n",
    "idxs_train, idxs_test = split_data(LENDATA, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "tfidf = TfidfTransformer()\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class to hold the data (similar to Dataset class in torch)\n",
    "# Do I want to put bag of words and tfidf at this stage?? probably.\n",
    "class MSDataset(object):\n",
    "    def __init__(self, data, labels, idxs_train, idxs_test):\n",
    "        self.train_set = [data[i] for i in idxs_train]\n",
    "        self.train_labels = [labels[i] for i in idxs_train]\n",
    "        self.test_set = [data[i] for i in idxs_test]\n",
    "        self.test_labels = [labels[i] for i in idxs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = MSDataset(dataset, data_label, idxs_train, idxs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(full_dataset, open('dataset.sav','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [0,1,2,3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to a Bag-of-Words model and using Tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bow.fit_transform(full_dataset.train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5242, 13462)\n",
      "(1311, 13462)\n"
     ]
    }
   ],
   "source": [
    "train_vectors = bow.transform(full_dataset.train_set)\n",
    "test_vectors = bow.transform(full_dataset.test_set)\n",
    "# model = MLPClassifier(hidden_layer_sizes=(30,),batch_size=16,max_iter=20)\n",
    "# classifier = make_pipeline(vectorizer,model)\n",
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_model = LogisticRegression()\n",
    "regression_model.fit(train_vectors, full_dataset.train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5179252479023646\n"
     ]
    }
   ],
   "source": [
    "score = regression_model.score(test_vectors, full_dataset.test_labels)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPMS\n"
     ]
    }
   ],
   "source": [
    "unknown_vectors = bow.transform(unknown)\n",
    "unknown_predictions = regression_model.predict(unknown_vectors)\n",
    "print(Class_label_correspondances[unknown_predictions[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model to be used by the app\n",
    "model_sav = 'regression_model_bow.sav'\n",
    "pickle.dump(regression_model, open(model_sav, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = vectorizer.transform(full_dataset.train_set)\n",
    "test_vectors = vectorizer.transform(full_dataset.test_set)\n",
    "# model = MLPClassifier(hidden_layer_sizes=(30,),batch_size=16,max_iter=20)\n",
    "# classifier = make_pipeline(vectorizer,model)\n",
    "print(train_vectors.shape)\n",
    "print(test_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to use keras NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.expanduser('~'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the 20 News Groups dataset to use as a negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = pickle.load(open('twenty_train.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_text = []\n",
    "for i in range(len(twenty_train.target)):\n",
    "    if twenty_train.target[i] == 19:\n",
    "        religion_text.append(twenty_train.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the OneClassSVM on the MS free text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_class_svm = OneClassSVM(nu=0.1,kernel='rbf',gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneClassSVM(cache_size=200, coef0=0.0, degree=3, gamma=0.1, kernel='rbf',\n",
       "            max_iter=-1, nu=0.1, random_state=None, shrinking=True, tol=0.001,\n",
       "            verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_class_svm.fit(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on the train set to see how it predicts on itself\n",
    "pred = one_class_svm.predict(train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [1555 3687]\n"
     ]
    }
   ],
   "source": [
    "unique, count = np.unique(pred, return_counts=True)\n",
    "print(unique, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing on the test set (which should all be true btw)\n",
    "pred_test = one_class_svm.predict(test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [776 535]\n"
     ]
    }
   ],
   "source": [
    "unique1, count1 = np.unique(pred_test, return_counts=True)\n",
    "print(unique1, count1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming the random religion set into vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# religion_and_test = full_dataset.test_set + religion_text\n",
    "# religion_and_test_labels = np.concatenate(np.ones_like(np.array(full_dataset.test_set)),np.zeros_like(np.array(religion_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# religion_and_test_vectors = vectorizer.transform(religion_and_test)\n",
    "# print(religion_and_test_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_test = one_class_svm.predict(religion_and_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique2, count2 = np.unique(final_test, return_counts=True)\n",
    "# print(unique2, count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
